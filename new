#NEW

rm(list=ls())
set.seed(1)
setwd("C:\\Users\\pc\\Dropbox\\random_number_test")

#install.packages("readxl")
#install.packages("ISLR")
#install.packages("caTools")

library(readxl)
library(ISLR)
library(stats)
library(caTools)


#reading all sheets
all_sheets<-read_excel("roulette.xls", na= "NA")

read_excel_allsheets <- function(filename) {
  sheets <- readxl::excel_sheets(filename)
  x <-    lapply(sheets, function(X) readxl::read_excel(filename, sheet = X))
  names(x) <- sheets
  x
}









#reading the particular sheets in excel
#feb_list_test <- as.matrix(all_sheets$`Jan 2015`)

#removing na values
#feb_list_test<-feb_list_test[!is.na(feb_list_test)]



#all data
cc1 <- c()

for(months_a in names(all_sheets))
  
{
  #selecting each month
  bb <- data.frame(subset(all_sheets, select = months_a))
  month_test <- as.matrix(bb)
  
  #removing na values
  month_test<-month_test[!is.na(month_test)]
  cc1 <- append(cc1, month_test)
  
}


feb_list_test<-cc1


#making the data frame with features past numbers
nodata <- data.frame(day_1= numeric(length(feb_list_test[2:(length(feb_list_test)-9)])), 
                     day_2= numeric(length(feb_list_test[2:(length(feb_list_test)-9)])),
                     day_4= numeric(length(feb_list_test[2:(length(feb_list_test)-9)])), 
                     day_0 = numeric(length(feb_list_test[2:(length(feb_list_test)-9)])))
nodata$day_2 <- feb_list_test[3:(length(feb_list_test)-8)]
nodata$day_4 <- feb_list_test[5:(length(feb_list_test)-6)]
nodata$day_1 <- feb_list_test[2:(length(feb_list_test)-9)]
nodata$day_0 <- feb_list_test[1:(length(feb_list_test)-10)]
nodata$day_mod_0 <- feb_list_test[1:(length(feb_list_test)-10)]

#for predicting from 0 to 36 past numbers
nums_37 <- data.frame(day_1=0:36)
nums_37$day_1 <- as.factor(nums_37$day_1)


#list of prediction MEAN accuracy for model selection
corr_list <- c()

#list for classifiers
cc <- list()

#list for difference error of train and test
ab<-c()

#list for sets
list_training <- list()
list_test <- list()





#numbers with high pvalues
list_ccc<-list()
pval<-as.list(rep(0,times=37))
for(j in 0:36)
{
  
  
  #setting a from 0:36
  a = j
  
  
  #checking the error prediction for the model in 2000 tries vector of prediction accuracy
  corr_<-c()
  corr_train<-c()
  
  #checking if tehre are other possibilities
  test_<-0
  for(i in 1:2000)
  {
    print(i)
    print(j)
    #making the target number as 1 and other numbers as zero
    nodata$day_mod_0[nodata$day_0==a]=1
    nodata$day_mod_0[nodata$day_0!=a]=0
    xx <- nodata[nodata$day_mod_0==1,]
    yy <- nodata[nodata$day_mod_0!=1,]
    #yy<-yy[sample(1:length(yy[,1]), round(0.75*length(xx[,1]))),]
    #dataset <- rbind(xx, yy)
    split_xx = sample.split(xx$day_mod_0, SplitRatio = 0.8)
    
    
    xx_training_set = subset(xx, split_xx == TRUE)
    xx_test_set = subset(xx, split_xx == FALSE)
    yy_training_set = yy[sample(1:length(yy[,1]), round(1.1*length(xx_training_set[,1]))),]
    yy_test_set = yy[!rownames(yy) %in% rownames(yy_training_set),]
    yy_test_set = yy_test_set[sample(1:length(yy_test_set[,1]), round(36*length(xx_test_set[,1])),replace = TRUE),]
    dataset_training_set <- rbind(xx_training_set, yy_training_set)
    dataset_test_set <- rbind(xx_test_set, yy_test_set)
    #dataset <- dataset[,c(1,3)]
    #dataset = read.csv('romnick.csv')
    
    
    # Encoding the target feature as factor for logistic
    dataset_training_set$day_mod_0 = factor(dataset_training_set$day_mod_0)
    dataset_training_set$day_1 = factor(dataset_training_set$day_1)
    dataset_training_set$day_2 = factor(dataset_training_set$day_2)
    dataset_training_set$day_4 = factor(dataset_training_set$day_4)
    dataset_test_set$day_mod_0 = factor(dataset_test_set$day_mod_0)
    dataset_test_set$day_1 = factor(dataset_test_set$day_1)
    dataset_test_set$day_2 = factor(dataset_test_set$day_2)
    dataset_test_set$day_4 = factor(dataset_test_set$day_4)
    nodata$day_1 = factor(nodata$day_1)
    nodata$day_2 = factor(nodata$day_2)
    nodata$day_4 = factor(nodata$day_4)
    
    
    # Splitting the dataset into the Training set and Test set
    #set.seed(123)
    #split = sample.split(dataset$day_mod_0, SplitRatio = 0.90)
    #training_set = subset(dataset, split == TRUE)
    #test_set = subset(dataset, split == FALSE)
    
    # cond_train <- dataset_training_set[,1]==0 | dataset_training_set[,1]==1
    # cond_test <- dataset_test_set[,1]==0 | dataset_test_set[,1]==1
    #    # classifier = glm(formula = day_mod_0 ~ day_1,
    #               family = binomial,
    #               data = dataset_training_set[cond_train,!colnames(dataset_training_set) %in% c("day_2","day_0", "day_4")])
    # 
    classifier = glm(formula = day_mod_0 ~ day_2,
                     family = binomial,
                     data = dataset_training_set[,!colnames(dataset_training_set) %in% c("day_1","day_0", "day_4")])
    # 
    
    #print(classifier)
    #avoiding some error
    classifier$xlevels[["day_1"]] <- union(classifier$xlevels[["day_1"]], levels(dataset_test_set$day_1))
    classifier$xlevels[["day_1"]] <- union(classifier$xlevels[["day_1"]], levels(dataset_training_set$day_1))
    classifier$xlevels[["day_2"]] <- union(classifier$xlevels[["day_2"]], levels(dataset_test_set$day_2))
    classifier$xlevels[["day_2"]] <- union(classifier$xlevels[["day_2"]], levels(dataset_training_set$day_2))
    classifier$xlevels[["day_4"]] <- union(classifier$xlevels[["day_4"]], levels(dataset_test_set$day_4))
    classifier$xlevels[["day_4"]] <- union(classifier$xlevels[["day_4"]], levels(dataset_training_set$day_4))
    
    
    
    
    # Predicting the Test set results
    y_pred  = rep(0,times=length(dataset_test_set[,1]))
    y_pred_train = rep(0,times=length(dataset_training_set[,1]))
    # prob_pred = predict(classifier, type = 'response', newdata = dataset_test_set[cond_test,!colnames(dataset_test_set) %in% c("day_2","day_0", "day_4")])
    # train_pred = predict(classifier, type = 'response', newdata = dataset_training_set[cond_train,!colnames(dataset_training_set) %in% c("day_2","day_0", "day_4")])
    prob_pred = predict(classifier, type = 'response', newdata = dataset_test_set[,!colnames(dataset_test_set) %in% c("day_1","day_0", "day_4")])
    train_pred = predict(classifier, type = 'response', newdata = dataset_training_set[,!colnames(dataset_training_set) %in% c("day_1","day_0", "day_4")])
    
    # y_pred[cond_test] = ifelse(prob_pred > 0.5, 1, 0)
    # y_pred_train[cond_train] = ifelse(train_pred > 0.5, 1, 0)
    
    y_pred= ifelse(prob_pred > 0.5, 1, 0)
    y_pred_train = ifelse(train_pred > 0.5, 1, 0)
    
    
    # Making the Confusion Matrix
    # cm = table(dataset_test_set[cond_test,5], y_pred)
    # cm_train = table(dataset_training_set[cond_train,5], y_pred_train)
    
    cm = table(dataset_test_set[,5], y_pred)
    cm_train = table(dataset_training_set[,5], y_pred_train)
    
    #print(cm)
    print(summary(classifier)$coefficients)
    
    
    #print(summary(classifier))
    
    for(m in 1:length(summary(classifier)$coefficients[,4]))
    {
      if(as.numeric(summary(classifier)$coefficients[m,4])<0.05)
      {
        print("HELLO")
        pval[[m]]<-pval[[m]]+1
        
        
      }
    }
    
    
    
    #checking the dimension of confusion matrix and selecting the classifier
    if(dim(cm)[1]==2 && dim(cm)[2]==2 && dim(cm_train)[1]==2 && dim(cm_train)[2]==2)
    {
      corr_a <- (((cm[2,2])/(cm[2,2]+cm[1,2])))
      corr_a<-corr_a/(1/37)
      corr_<-append(corr_,corr_a)
      corr_b<- (((cm_train[2,2])/(cm_train[2,2]+cm_train[1,2])))
      corr_b<-corr_b/(1/2.1)
      corr_train<-append(corr_train,corr_b)
      
      print(corr_a)
      
      print(corr_b)
      
      
      print(cm[1,2]+cm[2,2])
      
      print(mean(corr_))
      print(mean(corr_train))
      #test_<-test_+1
      if((1/37)*corr_a>0.0285 && (1/2.1)*corr_b>0.515 && abs(corr_a-corr_b)<0.01)
      {
        #print(corr_a)
        #corr_<-append(corr_,(cm[2,2])/(cm[2,2]+cm[1,2]))
        #ab <- append(ab, abs(corr_a-corr_b))
        # cc[[j+1]]<-classifier
        # list_training[[j+1]] <- dataset_training_set
        # list_test[[j+1]] <- dataset_test_set
        # break
      }
      
    }
    
    if(dim(cm)[1]==1 && dim(cm)[2]==2)
    {
      
    }
    # if(dim(cm)[1]==2 && dim(cm)[2]==1 && dim(cm_train)[1]==2 && dim(cm_train)[2]==1)
    # {
    #   test_<-test_+1
    #   if(rownames(cm)[2]==colnames(cm)[1])
    #   {
    #     
    #   }
    #   if(rownames(cm)[1]==colnames(cm)[1] && rownames(cm_train)[1]==colnames(cm_train)[1])
    #   {
    #     corr_train<-append(corr_train,(cm_train[2,1])/(cm_train[1,1]+cm_train[2,1]))
    #     corr_<-append(corr_,(cm[2,1])/(cm[2,1]+cm[1,1]))
    #     corr_a <- (cm[2,1])/(cm[1,1]+cm[2,1])
    #     corr_b<- (cm_train[2,1])/(cm_train[1,1]+cm_train[2,1])
    #     if(abs(corr_a-corr_b)<0.05)
    #     {
    #       ab <- append(ab, abs(corr_a))
    #       #corr_<-append(corr_,(cm[2,1])/(cm[2,1]+cm[1,1]))
    #       cc[[j+1]]<-classifier
    #       break
    #     }
    #   }
    #   
    # }
    
    
    
  }
  
  
  plot(1:length(corr_),corr_,type="p", col="green", main=paste(mean(corr_train),mean(corr_), "increasing test set size",sep=" "))
  lines(1:length(corr_train),corr_train,type="p", col="red")
  lines(1:length(corr_train),rep(1, times=length(corr_train)),type="l", col="blue", lwd = 10)
  lines(1:length(corr_train),rep(mean(corr_train),times=length(corr_train)),type="p", col="red",lwd = 10)
  lines(1:length(corr_),rep(mean(corr_),times=length(corr_)),type="p", col="green",lwd = 10)
  
  
  #plot(1-(corr_train/0.50))
  #plot(1-(corr_/(1/37)))
  #corr_list <- append(corr_list, corr_)
  hist(corr_, main = paste("Test, For Even Data same length" , j, mean(corr_),sep=" "))
  hist(corr_train, main = paste("Train, For Even Data same length" , j, mean(corr_train),sep=" "))
  #plot(corr_-corr_train)
  
  #print(pval)
  ccc_max <- which.max(as.numeric(pval))
  ccc_2nd_max <- which(as.numeric(pval)==max(as.numeric(pval[as.numeric(pval)!=as.numeric(pval[ccc_max])])))
  ccc_3nd_max <- which(as.numeric(pval)==max(as.numeric(pval[as.numeric(pval)!=as.numeric(pval[ccc_max]) & as.numeric(pval)!=as.numeric(pval[ccc_2nd_max])])))
  list_ccc[[j+1]] <- list(ccc_max-1, ccc_2nd_max-1, ccc_3nd_max-1)
  print(list_ccc)
  
  
}




#1/37
kk_list<-c()
for(m in 1:37)
{
  whole_pred = predict(cc[[m]], type = 'response', newdata = nodata[,!colnames(nodata) %in% c("day_0")])
  whole_y_pred = ifelse(whole_pred > 0.5, 1, 0)
  kk<-table(nodata[,3], whole_y_pred)
  kk_list<-append(kk_list,(kk[2,2])/(kk[2,2]+kk[1,2]))
}
mean(kk_list)





#whole
ab
kk_list<-c()
for(m in 1:37)
{
  whole_pred = predict(cc[[m]], type = 'response', newdata = nodata[,!colnames(nodata) %in% c("day_0")])
  whole_y_pred = ifelse(whole_pred > 0.5, 1, 0)
  kk<-table(nodata[,3], whole_y_pred)
  kk_list<-append(kk_list,(kk[2,2]+kk[1,1])/(sum(kk)))
}
mean(kk_list)






#writing the matrix for each model (depending on the number) for predicted values for each number
final_prob <- c()
for(k in 1:37)
{
  final_prob <-rbind(final_prob, predict(cc[[k]], type = 'response', newdata = nums_37))
}


write.table(final_prob, file="roullette_strag_odd_even.csv", sep=",")
kk_final <- c()



#strategy for choosing numbers from 0 to 36 depending  on the probability 0.5 or 0.6
for(m in 1:37)
{
  kk<-c()
  for(l in 1:37)
  {
    if(final_prob[l,m]>0.60)
    {
      kk<-append(kk,(l-1))
    }
  }
  kk_final <- append(kk_final, paste(kk, collapse=" "))
}

write.table(kk_final, file="rr3.csv", sep = ", ")

pos_ev<-c() # note 0:36


for(n in 0:36)
{
  c_1<-final_prob[,1+n]/sum(final_prob[,1+n])
  if((abs(sum(c_1[2:19])-sum(c_1[20:37])) - c_1[1])>0)
  {
    pos_ev<-append(pos_ev, n)
  }
}
ev_<-c()
pos_ev_deci<-c()
for(n in 0:36)
{
  c_1<-final_prob[,1+n]/sum(final_prob[,1+n])
  if((abs(sum(c_1[2:19])-sum(c_1[20:37])) - c_1[1])>0)
  {
    ev_ <- append(ev_, (abs(sum(c_1[2:19])-sum(c_1[20:37])) - c_1[1]))
    if((sum(c_1[2:19])-sum(c_1[20:37]))>0)
    {
      pos_ev_deci<-append(pos_ev_deci, "down")
    }
    else
    {
      pos_ev_deci<-append(pos_ev_deci, "up")
    }
  }
}




df = data.frame(pos_ev, pos_ev_deci, ev_)
write.table(df, file="rr5.csv", sep = ", ")
